{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Answer: A Support Vector Machine (SVM) is a powerful and versatile supervised machine learning algorithm primarily used for classification and regression tasks. It's particularly well-known for its effectiveness in classification, especially in scenarios involving high-dimensional data or complex decision boundaries.\n",
        "\n",
        "How a Support Vector Machine (SVM) Works:\n",
        "The core idea behind SVM is to find the \"best\" possible decision boundary, known as a hyperplane, that separates data points belonging to different classes. Here's a breakdown of how it works:\n",
        "\n",
        "Representing Data Points:\n",
        "\n",
        "Each data point in your dataset is treated as a point in an n-dimensional space, where 'n' is the number of features (or dimensions) in your data.\n",
        "\n",
        "For example, if you have two features (like height and weight), each data point would be represented as a point on a 2D plane. If you have three features, it's a point in 3D space, and so on.\n",
        "\n",
        "The Hyperplane:\n",
        "\n",
        "The hyperplane is the decision boundary that separates the different classes.\n",
        "\n",
        "In a 2D space, the hyperplane is a line.\n",
        "\n",
        "In a 3D space, it's a plane.\n",
        "\n",
        "In higher dimensions, it's a hyperplane.\n",
        "\n",
        "Maximizing the Margin:\n",
        "\n",
        "The key objective of SVM is to find the hyperplane that has the largest possible margin between the two classes.\n",
        "\n",
        "The margin is the distance between the hyperplane and the closest data points from each class.\n",
        "\n",
        "Maximizing this margin is crucial because it generally leads to better generalization performance on new, unseen data. A larger margin indicates a more robust and less prone-to-overfitting model.\n",
        "\n",
        "Support Vectors:\n",
        "\n",
        "The data points that lie closest to the hyperplane and effectively define the margin are called support vectors.\n",
        "\n",
        "These support vectors are critical because they are the only data points that directly influence the position and orientation of the hyperplane. All other data points can be removed, and the hyperplane would remain the same. This makes SVM memory efficient.\n",
        "\n",
        "Linear vs. Non-Linear Separation:\n",
        "\n",
        "Linearly Separable Data (Hard Margin SVM):\n",
        "\n",
        "If the data points can be perfectly separated by a straight line (or hyperplane), the SVM aims to find the hyperplane with the maximum margin, ensuring no misclassifications in the training data. This is known as a \"hard margin\" SVM.\n",
        "\n",
        "Non-Linearly Separable Data (Soft Margin SVM and Kernel Trick):\n",
        "\n",
        "In many real-world scenarios, data is not perfectly linearly separable. In such cases, a \"soft margin\" SVM is used. This allows for some misclassifications or violations of the margin to achieve better generalization. A regularization parameter (C) is introduced to control the trade-off between maximizing the margin and minimizing misclassifications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For highly non-linear data, SVM employs a powerful technique called the \"kernel trick.\"\n",
        "\n",
        "The kernel trick implicitly maps the original data into a higher-dimensional feature space where it becomes linearly separable.\n",
        "\n",
        "Instead of explicitly calculating the coordinates in this higher-dimensional space (which can be computationally expensive), kernel functions (e.g., polynomial, Radial Basis Function - RBF, sigmoid) calculate the dot product of the data points in this transformed space. This allows SVM to find a linear decision boundary in the higher-dimensional space, which corresponds to a non-linear boundary in the original input space.\n",
        "\n",
        "\n",
        "Classification:\n",
        "\n",
        "Once the optimal hyperplane is found, new, unseen data points are classified based on which side of the hyperplane they fall.\n",
        "\n",
        "In summary, SVM works by:\n",
        "\n",
        "Finding the optimal hyperplane that best separates the different classes in the feature space.\n",
        "\n",
        "Maximizing the margin between the hyperplane and the closest data points (support vectors) from each class.\n",
        "\n",
        "Utilizing the kernel trick to handle non-linearly separable data by implicitly mapping it to a higher-dimensional space where linear separation is possible.\n",
        "\n",
        "SVMs are widely used in various applications, including image classification, natural language processing, bioinformatics, and anomaly detection, due to their robustness, effectiveness with high-dimensional data, and ability to handle complex decision boundaries."
      ],
      "metadata": {
        "id": "8bLJgEkiCh1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Answer: The distinction between Hard Margin and Soft Margin SVM lies in how strictly they enforce the separation of classes and handle misclassifications and outliers.\n",
        "\n",
        "Hard Margin SVM\n",
        "Concept:\n",
        "A Hard Margin SVM aims to find a hyperplane that perfectly separates the data points of different classes without any misclassifications. It seeks the largest possible margin such that all training data points are on the correct side of the margin.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Strict Separation: Requires all data points to be correctly classified and lie outside the margin. No training errors are tolerated.\n",
        "\n",
        "Applicability: Only applicable when the data is perfectly linearly separable. If even a single data point overlaps or is an outlier, a hard margin SVM cannot find a feasible separating hyperplane.\n",
        "\n",
        "\n",
        "Sensitivity to Outliers: Extremely sensitive to outliers. A single outlier can drastically change the position and orientation of the hyperplane, potentially leading to a very narrow margin or no solution at all. This makes the model less robust.\n",
        "\n",
        "Risk of Overfitting: Because it tries to perfectly classify every training point, it can lead to overfitting, especially in the presence of noise or outliers. The model might learn the training data too well, failing to generalize to unseen data.\n",
        "\n",
        "No Regularization Parameter: There's no explicit regularization parameter to control misclassifications; the goal is absolute separation.\n",
        "\n",
        "When to use:\n",
        "Hard Margin SVM is ideal for perfectly clean, linearly separable datasets where you are confident that there are no outliers or overlapping data points. These scenarios are rare in real-world data.\n",
        "\n",
        "Soft Margin SVM\n",
        "Concept:\n",
        "A Soft Margin SVM introduces a degree of tolerance for misclassifications and margin violations. It aims to find a hyperplane that maximizes the margin while allowing some training errors. This is achieved by introducing slack variables (ξ\n",
        "i\n",
        "​\n",
        " ) for each data point and a regularization parameter (C).\n",
        "\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Tolerance for Errors: Allows some data points to be misclassified or to fall within the margin (i.e., violate the margin constraints).\n",
        "\n",
        "Slack Variables (ξ\n",
        "i\n",
        "​\n",
        " ):\n",
        "\n",
        "ξ\n",
        "i\n",
        "​\n",
        " =0: Data point is correctly classified and outside the margin.\n",
        "\n",
        "0<ξ\n",
        "i\n",
        "​\n",
        " <1: Data point is correctly classified but falls within the margin.\n",
        "\n",
        "ξ\n",
        "i\n",
        "​\n",
        " ≥1: Data point is misclassified (on the wrong side of the hyperplane).\n",
        "\n",
        "Regularization Parameter (C):\n",
        "\n",
        "This is the most crucial difference. 'C' controls the trade-off between maximizing the margin and minimizing the training errors (margin violations).\n",
        "\n",
        "High C value: Penalizes misclassifications and margin violations heavily. This pushes the SVM towards a hard margin behavior, potentially leading to a narrower margin and a model that fits the training data more closely (higher risk of overfitting).\n",
        "\n",
        "Low C value: Allows more misclassifications and margin violations. This results in a wider margin and a more robust model that generalizes better to unseen data (lower risk of overfitting).\n",
        "\n",
        "Robustness to Outliers: Much more robust to outliers and noisy data because it doesn't force a perfect separation. Outliers will incur a penalty, but they won't necessarily dictate the entire hyperplane.\n",
        "\n",
        "Applicability: Can handle non-linearly separable data (when combined with the kernel trick) and real-world datasets that often contain noise and overlap.\n",
        "\n",
        "Better Generalization: By allowing some errors, the model often achieves better generalization performance on unseen data, as it focuses on finding a more generalizable separation rather than an overly strict one tied to every single training point."
      ],
      "metadata": {
        "id": "4vO7PGvjC7y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Answer: The Kernel Trick is a fundamental concept in Support Vector Machines (SVMs) that allows them to efficiently handle non-linear classification problems.\n",
        "\n",
        "What is the Kernel Trick?\n",
        "Imagine you have a dataset where the classes are not linearly separable in their original feature space. For example, data points might be arranged in concentric circles, where a straight line simply cannot separate them.\n",
        "\n",
        "One way to deal with this is to explicitly transform the original data into a higher-dimensional space where it does become linearly separable. You could define new features that are combinations or transformations of the original ones (e.g., if you have features x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " , you might create new features like x\n",
        "1\n",
        "2\n",
        "​\n",
        " ,x\n",
        "2\n",
        "2\n",
        "​\n",
        " ,x\n",
        "1\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " ). In this new, higher-dimensional \"feature space,\" you might then be able to find a linear hyperplane to separate the classes.\n",
        "\n",
        "However, explicitly transforming the data can be computationally very expensive, especially if the new feature space is extremely high-dimensional or even infinite-dimensional. This is where the Kernel Trick comes in.\n",
        "\n",
        "The \"trick\" is that SVM's optimization problem (specifically, the dual form) only involves dot products of data points. Instead of explicitly calculating the transformed features ϕ(x) for each data point x and then computing their dot product ϕ(x\n",
        "i\n",
        "​\n",
        " )⋅ϕ(x\n",
        "j\n",
        "​\n",
        " ), the kernel trick allows us to use a kernel function K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " ) that directly computes this dot product in the higher-dimensional space without ever explicitly performing the transformation ϕ.\n",
        "\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=ϕ(x\n",
        "i\n",
        "​\n",
        " )⋅ϕ(x\n",
        "j\n",
        "​\n",
        " )\n",
        "So, in essence, the Kernel Trick:\n",
        "\n",
        "Avoids explicit mapping: It allows SVM to work in a high-dimensional feature space without the computational burden of actually transforming the data points into that space.\n",
        "\n",
        "Calculates similarity directly: It computes the similarity (dot product) between data points as if they were already in the higher-dimensional space.\n",
        "\n",
        "Enables non-linear decision boundaries: By implicitly operating in a higher dimension, a linear decision boundary in that higher space corresponds to a non-linear decision boundary in the original, lower-dimensional space.\n",
        "\n",
        "Example of a Kernel and its Use Case:\n",
        "One of the most widely used kernel functions is the Radial Basis Function (RBF) Kernel, also known as the Gaussian Kernel.\n",
        "\n",
        "Formula:\n",
        "The RBF kernel for two data points x\n",
        "i\n",
        "​\n",
        "  and x\n",
        "j\n",
        "​\n",
        "  is defined as:\n",
        "\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∥x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∥\n",
        "2\n",
        " )\n",
        "\n",
        "where:\n",
        "\n",
        "∥x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∥\n",
        "2\n",
        "  is the squared Euclidean distance between the two data points in the original input space.\n",
        "\n",
        "γ (gamma) is a positive parameter that controls the \"reach\" or influence of each training example. A small γ means a large radius of influence, while a large γ means a small radius.\n",
        "\n",
        "How it works (intuition):\n",
        "The RBF kernel measures the similarity between two points based on their distance. If two points are very close to each other, their RBF kernel value will be close to 1 (meaning they are very similar in the transformed space). As the distance between them increases, the kernel value approaches 0, indicating less similarity. This effectively creates \"bumps\" or \"hills\" around each data point in the higher-dimensional space.\n",
        "\n",
        "Use Case:\n",
        "The RBF kernel is a general-purpose kernel and is particularly effective when:\n",
        "\n",
        "The relationship between classes is highly non-linear: It can map data into an infinite-dimensional space, allowing it to capture complex, intricate decision boundaries that are not easily described by simple polynomial functions.\n",
        "\n",
        "You have no prior knowledge about the data's structure: Because of its flexibility, it's often the first non-linear kernel to try when you suspect non-linear relationships but aren't sure of their specific form.\n",
        "\n",
        "Example Scenario:\n",
        "Consider a medical diagnosis problem where you're trying to classify patients as having a certain disease (Class 1) or not (Class 0) based on various symptoms and test results. It's highly unlikely that a simple linear relationship exists between these features and the disease outcome. For instance, a combination of mild symptoms might indicate the disease, while individually they don't, or there might be complex interactions between genetic markers and environmental factors.\n",
        "\n",
        "In such a scenario, the RBF kernel would be an excellent choice for an SVM. It would allow the SVM to implicitly learn a highly flexible, non-linear decision boundary in the multi-dimensional feature space, effectively separating the diseased and non-diseased patient groups, even if their relationship in the original symptom space is complex and intertwined. This makes the RBF kernel suitable for tasks like:\n",
        "\n",
        "Image classification: Recognizing handwritten digits or objects where pixel values have complex non-linear relationships.\n",
        "\n",
        "Bioinformatics: Analyzing gene expression data for disease prediction.\n",
        "\n",
        "Financial forecasting: Predicting market trends based on a multitude of interacting economic indicators.\n",
        "\n",
        "In practice, the RBF kernel is one of the most frequently used kernels due to its strong performance across a wide range of real-world datasets."
      ],
      "metadata": {
        "id": "V9HrRVJ6DOlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Answer: A Naïve Bayes Classifier is a probabilistic machine learning algorithm used for classification tasks. It's based on Bayes' Theorem with a crucial \"naïve\" assumption about the independence of features.\n",
        "\n",
        "How it Works (Based on Bayes' Theorem):\n",
        "Bayes' Theorem provides a way to calculate the probability of a hypothesis (H) given some evidence (E):\n",
        "\n",
        "P(H∣E)=\n",
        "P(E)\n",
        "P(E∣H)⋅P(H)\n",
        "​\n",
        "\n",
        "In the context of classification, this translates to:\n",
        "\n",
        "P(Class∣Features)=\n",
        "P(Features)\n",
        "P(Features∣Class)⋅P(Class)\n",
        "​\n",
        "\n",
        "Let's break down each term:\n",
        "\n",
        "P(Class∣Features): This is the posterior probability – the probability that an instance belongs to a certain class given its observed features. This is what we want to predict.\n",
        "\n",
        "P(Features∣Class): This is the likelihood – the probability of observing these specific features given that the instance belongs to a particular class.\n",
        "\n",
        "P(Class): This is the prior probability – the probability of a class occurring independently of any features. It's simply the proportion of that class in the training data.\n",
        "\n",
        "P(Features): This is the evidence or marginal probability – the probability of observing the given features across all classes. For classification, this term is often constant for all classes, so it acts as a scaling factor and doesn't affect which class has the highest posterior probability; thus, it can sometimes be ignored for comparison purposes.\n",
        "\n",
        "The Naïve Bayes classifier calculates P(Class∣Features) for each possible class and then predicts the class with the highest probability.\n",
        "\n",
        "Why is it called “Naïve”?\n",
        "The \"naïve\" part comes from the fundamental and strong assumption it makes:\n",
        "\n",
        "Conditional Independence of Features:\n",
        "The Naïve Bayes classifier assumes that all features are conditionally independent of each other, given the class label.\n",
        "\n",
        "In simpler terms, this means that the presence or absence of a particular feature does not affect the presence or absence of any other feature, assuming you already know the class of the instance.\n",
        "\n",
        "Mathematically, if we have features F\n",
        "1\n",
        "​\n",
        " ,F\n",
        "2\n",
        "​\n",
        " ,…,F\n",
        "n\n",
        "​\n",
        " , the naive assumption allows us to simplify the likelihood term as follows:\n",
        "\n",
        "P(Features∣Class)=P(F\n",
        "1\n",
        "​\n",
        " ,F\n",
        "2\n",
        "​\n",
        " ,…,F\n",
        "n\n",
        "​\n",
        " ∣Class)≈P(F\n",
        "1\n",
        "​\n",
        " ∣Class)⋅P(F\n",
        "2\n",
        "​\n",
        " ∣Class)⋅⋯⋅P(F\n",
        "n\n",
        "​\n",
        " ∣Class)\n",
        "Why is this assumption \"naïve\"?\n",
        "\n",
        "In reality, features are rarely truly independent. For example:\n",
        "\n",
        "Email Spam Classification: If you're classifying an email as spam, and one feature is \"contains the word 'Viagra'\" and another is \"contains a link to a suspicious website,\" these features are highly likely to appear together if the email is spam. They are not independent.\n",
        "\n",
        "Medical Diagnosis: If classifying a patient based on symptoms, \"fever\" and \"cough\" are often correlated symptoms when a patient has the flu. They are not independent.\n",
        "\n",
        "Weather Prediction: \"Temperature\" and \"humidity\" are often correlated.\n",
        "\n",
        "Despite this unrealistic assumption, Naïve Bayes classifiers often perform surprisingly well in practice, especially in text classification (like spam detection) and document categorization.\n",
        "\n",
        "Advantages of Naïve Bayes:\n",
        "\n",
        "Simple and Fast: Easy to implement and computationally very efficient, making it suitable for large datasets.\n",
        "\n",
        "Good for High-Dimensional Data: Performs well with a large number of features.\n",
        "\n",
        "Requires Less Training Data: Can perform reasonably well even with a relatively small amount of training data compared to more complex algorithms.\n",
        "\n",
        "Handles Continuous and Discrete Data: Can be adapted for both types of data (e.g., Gaussian Naïve Bayes for continuous data, Multinomial Naïve Bayes for count data).\n",
        "\n",
        "While the \"naïve\" assumption might seem like a fatal flaw, its simplicity often leads to robust and effective models in many real-world applications."
      ],
      "metadata": {
        "id": "oqxeBYVKD5nG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Answer: Naïve Bayes classifiers have different variants depending on the distribution assumption for the features. The three most common ones are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "Description:\n",
        "\n",
        "Assumes Gaussian (Normal) Distribution: This variant assumes that the continuous numerical features associated with each class follow a Gaussian (normal) distribution.\n",
        "\n",
        "Calculates Mean and Variance: During the training phase, for each class and each feature, Gaussian Naïve Bayes calculates the mean (μ) and variance (σ\n",
        "2\n",
        " ) of the feature values belonging to that class. These two parameters are then used to estimate the probability density function (PDF) for new data points using the Gaussian probability formula:\n",
        "\n",
        "P(x∣Class)=\n",
        "2πσ\n",
        "2\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "1\n",
        "​\n",
        " exp(−\n",
        "2σ\n",
        "2\n",
        "\n",
        "(x−μ)\n",
        "2\n",
        "\n",
        "​\n",
        " )\n",
        "Handles Continuous Data: It's specifically designed for features that are continuous (e.g., measurements like height, weight, temperature, sensor readings).\n",
        "\n",
        "When to Use:\n",
        "\n",
        "Continuous Data: Use Gaussian Naïve Bayes when your features are continuous and you assume (or can approximate) that they are normally distributed within each class.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Predicting a person's weight based on height and age.\n",
        "\n",
        "Classifying a flower species based on sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "Predictive maintenance: Predicting equipment failure based on continuous sensor data (e.g., temperature, vibration levels).\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "Description:\n",
        "\n",
        "Assumes Multinomial Distribution: This variant is suitable for discrete features that represent counts or frequencies. It models the probability of observing a particular set of counts for a fixed number of trials.\n",
        "\n",
        "Counts Occurrences: It works by counting the occurrences of features. For example, in text classification, it counts how many times each word appears in a document.\n",
        "\n",
        "Suitable for Text Data: It's particularly well-suited for text classification problems where features are typically word counts or frequencies (e.g., Bag-of-Words model).\n",
        "\n",
        "When to Use:\n",
        "\n",
        "Discrete Count Data: Use Multinomial Naïve Bayes when your features represent counts or frequencies of events.\n",
        "\n",
        "Text Classification: This is its primary and most common use case.\n",
        "\n",
        "Spam detection: Classifying emails as \"spam\" or \"not spam\" based on the frequency of certain words.\n",
        "\n",
        "Sentiment analysis: Determining if a movie review is positive, negative, or neutral based on word counts.\n",
        "\n",
        "Document categorization: Assigning news articles to categories like \"sports,\" \"politics,\" or \"technology\" based on word frequencies.\n",
        "\n",
        "Fractional Counts: While it technically requires integer counts, it can also work well with fractional counts like TF-IDF (Term Frequency-Inverse Document Frequency) values, which normalize word counts.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "Description:\n",
        "\n",
        "Assumes Bernoulli Distribution: This variant is designed for binary or boolean features. It models the presence or absence of a feature, rather than its count.\n",
        "\n",
        "Binary Features: Each feature is treated as a binary variable, meaning it can only take on two values (e.g., 0 or 1, True or False, present or absent).\n",
        "\n",
        "Focus on Presence/Absence: Instead of counting how many times a word appears, Bernoulli Naïve Bayes only cares if a word is present or absent in a document.\n",
        "\n",
        "When to Use:\n",
        "\n",
        "Binary Data: Use Bernoulli Naïve Bayes when your features are binary (0 or 1).\n",
        "\n",
        "Text Classification (Presence/Absence): Also used for text classification, but specifically when the presence or absence of a word is more important than its frequency.\n",
        "\n",
        "Document classification where rare words might be highly indicative: For instance, if a specific rare technical term is present, it might strongly suggest a document belongs to a certain technical category, regardless of how many times it appears.\n",
        "\n",
        "Spam detection where the existence of a suspicious keyword matters most.\n",
        "\n",
        "Other Binary Feature Sets: Any dataset where all features are intrinsically binary (e.g., \"has disease X\" (yes/no), \"is smoker\" (yes/no)).\n",
        "\n",
        "Image Classification (Binary Pixels): Can be used for simple image classification where pixels are represented as binary values (e.g., black or white).\n",
        "\n",
        "Key Takeaway for Choosing:\n",
        "\n",
        "The choice of Naïve Bayes variant primarily depends on the nature of your features:\n",
        "\n",
        "Continuous numerical features: → Gaussian Naïve Bayes\n",
        "\n",
        "Discrete count/frequency features: → Multinomial Naïve Bayes\n",
        "\n",
        "Binary (presence/absence) features: → Bernoulli Naïve Bayes\n",
        "\n",
        "In many practical text classification tasks, Multinomial Naïve Bayes is often preferred because word frequencies usually provide more information than just presence/absence. However, Bernoulli Naïve Bayes can be effective when the presence of a feature itself is a strong indicator, regardless of its repetition."
      ],
      "metadata": {
        "id": "CErfsAZ7EOjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Uhqeo0NJEbyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading the Iris dataset...\")\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target # Target labels\n",
        "\n",
        "# Print some basic information about the dataset\n",
        "print(f\"Dataset loaded: {iris.DESCR.splitlines()[0]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Target classes: {iris.target_names}\")\n",
        "\n",
        "print(\"\\nSplitting the dataset into training and testing sets (80% train, 20% test)...\")\n",
        "# Split the dataset into training and testing sets\n",
        "# random_state ensures reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "print(\"\\nTraining an SVM Classifier with a linear kernel...\")\n",
        "# Initialize the SVM classifier with a linear kernel\n",
        "# The 'C' parameter (regularization) is set to 1.0 by default, which is a good starting point.\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the model using the training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nModel training complete. Making predictions on the test set...\")\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nRetrieving Support Vectors...\")\n",
        "# Get the support vectors\n",
        "# support_vectors_ is an attribute of SVC that holds the support vectors\n",
        "support_vectors = svm_model.support_vectors_\n",
        "\n",
        "# Print the number of support vectors\n",
        "print(f\"Number of Support Vectors: {len(support_vectors)}\")\n",
        "\n",
        "# Print the support vectors themselves\n",
        "# For larger datasets, printing all support vectors might be too much,\n",
        "# so we'll print a sample or just their count.\n",
        "# For Iris, printing all is usually fine as it's a small dataset.\n",
        "print(\"First 5 Support Vectors (if available):\")\n",
        "for i, sv in enumerate(support_vectors[:5]):\n",
        "    print(f\"  Support Vector {i+1}: {sv}\")\n",
        "\n",
        "if len(support_vectors) > 5:\n",
        "    print(\"...\")\n",
        "    print(f\"Last Support Vector: {support_vectors[-1]}\")\n",
        "\n",
        "print(\"\\nProgram finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LERIMwgCEj-V",
        "outputId": "4ec5e20b-9133-49b3-f821-626cf244540f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the Iris dataset...\n",
            "Dataset loaded: .. _iris_dataset:\n",
            "Number of samples: 150\n",
            "Number of features: 4\n",
            "Target classes: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Splitting the dataset into training and testing sets (80% train, 20% test)...\n",
            "Training set size: 120 samples\n",
            "Testing set size: 30 samples\n",
            "\n",
            "Training an SVM Classifier with a linear kernel...\n",
            "\n",
            "Model training complete. Making predictions on the test set...\n",
            "Model Accuracy: 1.0000\n",
            "\n",
            "Retrieving Support Vectors...\n",
            "Number of Support Vectors: 25\n",
            "First 5 Support Vectors (if available):\n",
            "  Support Vector 1: [4.8 3.4 1.9 0.2]\n",
            "  Support Vector 2: [5.1 3.3 1.7 0.5]\n",
            "  Support Vector 3: [4.5 2.3 1.3 0.3]\n",
            "  Support Vector 4: [5.6 3.  4.5 1.5]\n",
            "  Support Vector 5: [5.4 3.  4.5 1.5]\n",
            "...\n",
            "Last Support Vector: [4.9 2.5 4.5 1.7]\n",
            "\n",
            "Program finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "uhko45CVEvPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading the Breast Cancer dataset...\")\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data  # Features\n",
        "y = cancer.target # Target labels (0 for malignant, 1 for benign)\n",
        "\n",
        "# Print some basic information about the dataset\n",
        "print(f\"Dataset loaded: {cancer.DESCR.splitlines()[0]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Target classes: {cancer.target_names}\")\n",
        "\n",
        "print(\"\\nSplitting the dataset into training and testing sets (80% train, 20% test)...\")\n",
        "# Split the dataset into training and testing sets\n",
        "# random_state ensures reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "print(\"\\nTraining a Gaussian Naïve Bayes model...\")\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb_model = GaussianNB()\n",
        "\n",
        "# Train the model using the training data\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nModel training complete. Making predictions on the test set...\")\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "# Print the classification report\n",
        "# This includes precision, recall, f1-score, and support for each class\n",
        "# and overall accuracy.\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
        "\n",
        "print(\"\\nProgram finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tUL-k3wE1w1",
        "outputId": "53477e60-ccd8-432f-cc6f-586b3db66846"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the Breast Cancer dataset...\n",
            "Dataset loaded: .. _breast_cancer_dataset:\n",
            "Number of samples: 569\n",
            "Number of features: 30\n",
            "Target classes: ['malignant' 'benign']\n",
            "\n",
            "Splitting the dataset into training and testing sets (80% train, 20% test)...\n",
            "Training set size: 455 samples\n",
            "Testing set size: 114 samples\n",
            "\n",
            "Training a Gaussian Naïve Bayes model...\n",
            "\n",
            "Model training complete. Making predictions on the test set...\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n",
            "\n",
            "Program finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Zd5BdO0CE9va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading the Wine dataset...\")\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data  # Features\n",
        "y = wine.target # Target labels\n",
        "\n",
        "# Print some basic information about the dataset\n",
        "print(f\"Dataset loaded: {wine.DESCR.splitlines()[0]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Target classes: {wine.target_names}\")\n",
        "\n",
        "print(\"\\nSplitting the dataset into training and testing sets (80% train, 20% test)...\")\n",
        "# Split the dataset into training and testing sets\n",
        "# random_state ensures reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "print(\"\\nSetting up GridSearchCV to find the best C and gamma for SVM...\")\n",
        "# Define the parameter grid for GridSearchCV\n",
        "# 'C' is the regularization parameter. Smaller C means wider margin, more misclassifications allowed.\n",
        "# 'gamma' defines how much influence a single training example has.\n",
        "# Larger gamma means closer influence, smaller gamma means farther influence.\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf'] # RBF kernel is typically used with C and gamma tuning\n",
        "}\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation\n",
        "# verbose=2 provides detailed output during the search\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
        "\n",
        "print(\"Starting GridSearchCV. This might take a moment...\")\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGridSearchCV complete.\")\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"\\n--- Best Hyperparameters ---\")\n",
        "print(f\"Best C: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best Gamma: {grid_search.best_params_['gamma']}\")\n",
        "print(f\"Best Kernel: {grid_search.best_params_['kernel']}\")\n",
        "\n",
        "# Get the best model found by GridSearchCV\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nMaking predictions on the test set using the best model...\")\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_best = best_svm_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Accuracy with Best Hyperparameters: {accuracy_best:.4f}\")\n",
        "\n",
        "print(\"\\nProgram finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMgxGZQCFDDQ",
        "outputId": "148776da-d72f-425f-9343-3f94a103e6c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the Wine dataset...\n",
            "Dataset loaded: .. _wine_dataset:\n",
            "Number of samples: 178\n",
            "Number of features: 13\n",
            "Target classes: ['class_0' 'class_1' 'class_2']\n",
            "\n",
            "Splitting the dataset into training and testing sets (80% train, 20% test)...\n",
            "Training set size: 142 samples\n",
            "Testing set size: 36 samples\n",
            "\n",
            "Setting up GridSearchCV to find the best C and gamma for SVM...\n",
            "Starting GridSearchCV. This might take a moment...\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "GridSearchCV complete.\n",
            "\n",
            "--- Best Hyperparameters ---\n",
            "Best C: 100\n",
            "Best Gamma: 0.001\n",
            "Best Kernel: rbf\n",
            "\n",
            "Making predictions on the test set using the best model...\n",
            "Accuracy with Best Hyperparameters: 0.8333\n",
            "\n",
            "Program finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "4n3JvivYFY96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading a synthetic text dataset (20 Newsgroups)...\")\n",
        "# Load a subset of the 20 Newsgroups dataset for simplicity\n",
        "# We'll use two categories to make it a binary classification problem for ROC-AUC\n",
        "categories = ['alt.atheism', 'soc.religion.christian']\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X = newsgroups_data.data  # Text content\n",
        "y = newsgroups_data.target # Target labels (0 for alt.atheism, 1 for soc.religion.christian)\n",
        "\n",
        "# Print some basic information about the dataset\n",
        "print(f\"Dataset loaded: {newsgroups_data.DESCR.splitlines()[0]}\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Target classes: {newsgroups_data.target_names}\")\n",
        "\n",
        "print(\"\\nSplitting the dataset into training and testing sets (80% train, 20% test)...\")\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)} documents\")\n",
        "print(f\"Testing set size: {len(X_test)} documents\")\n",
        "\n",
        "print(\"\\nConverting text data into numerical features using TF-IDF Vectorizer...\")\n",
        "# Convert text data into numerical feature vectors using TF-IDF\n",
        "# TF-IDF (Term Frequency-Inverse Document Frequency) is suitable for text data\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Limit features for performance\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Number of features after TF-IDF vectorization: {X_train_vec.shape[1]}\")\n",
        "\n",
        "print(\"\\nTraining a Multinomial Naïve Bayes model...\")\n",
        "# Initialize the Multinomial Naïve Bayes classifier\n",
        "# MultinomialNB is well-suited for discrete count data like text (TF-IDF values can be treated as counts)\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the model using the vectorized training data\n",
        "nb_model.fit(X_train_vec, y_train)\n",
        "\n",
        "print(\"\\nModel training complete. Calculating ROC-AUC score...\")\n",
        "\n",
        "# Predict probabilities for the positive class (class 1: soc.religion.christian)\n",
        "# roc_auc_score requires probabilities for the positive class\n",
        "y_proba = nb_model.predict_proba(X_test_vec)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Optional: Plot ROC Curve for visualization\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nProgram finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "sVh7PpyrFeWI",
        "outputId": "512c0ec7-3da9-4b33-86ec-1cdf1db59103"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading a synthetic text dataset (20 Newsgroups)...\n",
            "Dataset loaded: .. _20newsgroups_dataset:\n",
            "Number of samples: 1796\n",
            "Target classes: ['alt.atheism', 'soc.religion.christian']\n",
            "\n",
            "Splitting the dataset into training and testing sets (80% train, 20% test)...\n",
            "Training set size: 1436 documents\n",
            "Testing set size: 360 documents\n",
            "\n",
            "Converting text data into numerical features using TF-IDF Vectorizer...\n",
            "Number of features after TF-IDF vectorization: 5000\n",
            "\n",
            "Training a Multinomial Naïve Bayes model...\n",
            "\n",
            "Model training complete. Calculating ROC-AUC score...\n",
            "ROC-AUC Score: 0.9970\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Program finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "Answer: I will outline an approach for automatically classifying emails as Spam or Not Spam, addressing the challenges you've described and explaining the business impact.\n",
        "\n",
        "Email Spam Classification Solution\n",
        "1. Data Preprocessing\n",
        "Effective preprocessing is crucial for text-based classification, especially with diverse vocabulary and potential missing data.\n",
        "\n",
        "Text Vectorization:\n",
        "\n",
        "Choice: TF-IDF (Term Frequency-Inverse Document Frequency) is an excellent choice for vectorizing email text.\n",
        "\n",
        "Justification: TF-IDF assigns weights to words based on their frequency within a document (Term Frequency) and their rarity across the entire corpus (Inverse Document Frequency). This helps in:\n",
        "\n",
        "Handling Diverse Vocabulary: It naturally gives higher importance to words that are unique to specific emails or categories (e.g., \"Viagra\" in spam, \"meeting\" in legitimate emails) while downplaying common words like \"the\" or \"a\" that appear everywhere.\n",
        "\n",
        "Reducing Dimensionality (with max_features): As shown in the \"naive-bayes-text-roc-auc\" Canvas, TfidfVectorizer allows setting max_features, which can limit the vocabulary size to the most relevant terms, helping manage diverse vocabulary and computational resources.\n",
        "\n",
        "Representing Semantic Importance: It captures a better representation of a word's importance in a document relative to the entire dataset, which is more robust than simple word counts (Bag-of-Words) for diverse text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Lowercasing: Convert all text to lowercase to treat \"Spam\" and \"spam\" as the same word.\n",
        "\n",
        "Punctuation Removal: Remove punctuation marks as they typically don't contribute to classification.\n",
        "\n",
        "Stop Word Removal: Eliminate common words (e.g., \"is\", \"and\", \"the\") that carry little semantic meaning and can add noise. TfidfVectorizer has a built-in stop_words='english' option.\n",
        "\n",
        "Tokenization: Break down sentences into individual words or tokens.\n",
        "\n",
        "TF-IDF Transformation: Apply TfidfVectorizer to convert the cleaned text into numerical feature vectors.\n",
        "\n",
        "Handling Incomplete or Missing Data:\n",
        "\n",
        "Email Content: For missing email body text, depending on the context, you might:\n",
        "\n",
        "Impute with Empty String: Treat missing content as an empty string. The TF-IDF vectorizer would then assign zero values for all terms for such emails.\n",
        "\n",
        "Exclude: If a significant portion of the data is missing, or if missingness is correlated with the target variable, you might consider excluding such emails, but this should be done cautiously to avoid data loss.\n",
        "\n",
        "Metadata (if any): If there are other features like sender, subject line, or timestamp, and they have missing values:\n",
        "\n",
        "Categorical Missing Data: For sender/subject, use a \"Missing\" category or impute with the mode.\n",
        "\n",
        "Numerical Missing Data: For timestamps (if converted to numerical features), impute with mean, median, or a more sophisticated method like K-Nearest Neighbors imputation, or derive features like \"time of day\" which might be less prone to missingness.\n",
        "\n",
        "Consistency: Ensure consistent handling across all features.\n",
        "\n",
        "2. Model Choice and Justification\n",
        "Choice: Naïve Bayes Classifier (specifically, Multinomial Naïve Bayes)\n",
        "\n",
        "Justification:\n",
        "\n",
        "Text Classification Suitability: As discussed in the previous questions, Multinomial Naïve Bayes is inherently well-suited for text classification tasks, where features are typically word counts or TF-IDF values. It performs remarkably well despite its \"naïve\" independence assumption.\n",
        "\n",
        "Efficiency and Scalability: Email datasets can be very large. Naïve Bayes is computationally efficient and scales well to high-dimensional data (like TF-IDF vectors), making it fast to train and predict, which is critical for real-time spam filtering.\n",
        "\n",
        "Interpretability (to some extent): While not as directly interpretable as simpler models, the probabilities derived from Naïve Bayes can offer insights into which words are most indicative of spam or legitimate emails.\n",
        "\n",
        "Baseline Performance: It often provides a strong baseline performance that is hard to beat by more complex models without significant tuning.\n",
        "\n",
        "Comparison with SVM:\n",
        "\n",
        "SVM: While SVMs (especially with RBF kernels) are powerful for complex decision boundaries, they can be computationally more expensive and slower to train on very large text datasets compared to Naïve Bayes. Tuning SVMs (like C and gamma) can also be more involved. For high-dimensional sparse data like TF-IDF, linear SVMs can be very effective, but Multinomial Naïve Bayes often offers a good balance of performance and speed.\n",
        "\n",
        "Conclusion: For the initial phase of an email spam classification system, Multinomial Naïve Bayes offers a robust, efficient, and surprisingly accurate solution, especially given the nature of text data.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "Class imbalance (far more legitimate emails than spam) is a common challenge in spam detection and can lead to models that are biased towards the majority class.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Resampling Techniques (during training):\n",
        "\n",
        "Oversampling Minority Class (e.g., SMOTE): Synthetic Minority Over-sampling Technique (SMOTE) creates synthetic samples of the minority class (spam) rather than just duplicating existing ones. This helps the model learn the characteristics of spam more effectively.\n",
        "\n",
        "Undersampling Majority Class: Randomly removing samples from the majority class (legitimate emails) to balance the dataset. This can lead to loss of information, so it's often used with caution or in combination with oversampling.\n",
        "\n",
        "Combined Approaches: Using a combination like SMOTEENN (SMOTE followed by Edited Nearest Neighbors) or SMOTETomek.\n",
        "\n",
        "Class Weights (within the model):\n",
        "\n",
        "Many machine learning algorithms (including some Naïve Bayes implementations, though less common than in SVMs or tree-based models) allow assigning higher weights to the minority class during training. This makes the model penalize misclassifications of the minority class more heavily.\n",
        "\n",
        "Threshold Adjustment:\n",
        "\n",
        "Instead of using the default 0.5 probability threshold for classification, you can adjust it. If you want to reduce false negatives (spam emails getting through), you might lower the threshold for classifying an email as spam. This can be done after training the model.\n",
        "\n",
        "Recommendation: Start with SMOTE to oversample the spam emails in the training set. This is generally preferred over undersampling as it prevents information loss.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "Given the class imbalance and the critical nature of spam detection, standard accuracy alone is not sufficient.\n",
        "\n",
        "Key Metrics:\n",
        "\n",
        "Precision (for Spam Class): Of all emails classified as spam, what proportion were actually spam? High precision means fewer legitimate emails are wrongly flagged as spam (fewer false positives), which is crucial for user experience.\n",
        "\n",
        "Recall (for Spam Class): Of all actual spam emails, what proportion were correctly identified as spam? High recall means fewer spam emails slip through the filter (fewer false negatives), which is the primary goal of a spam filter.\n",
        "\n",
        "F1-Score (for Spam Class): The harmonic mean of precision and recall. It provides a single metric that balances both, especially useful in imbalanced datasets.\n",
        "\n",
        "ROC-AUC Score: As demonstrated in the \"naive-bayes-text-roc-auc\" Canvas, the Receiver Operating Characteristic Area Under the Curve (ROC-AUC) is an excellent metric for evaluating binary classifiers, especially with imbalanced datasets.\n",
        "\n",
        "Justification: ROC-AUC measures the model's ability to distinguish between classes across various classification thresholds. A higher AUC indicates better overall discrimination performance. It's robust to class imbalance because it considers both true positive rates and false positive rates.\n",
        "\n",
        "Confusion Matrix: Provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. This is essential for understanding where the model is making errors.\n",
        "\n",
        "Prioritization: For a spam filter, there's often a trade-off between precision and recall.\n",
        "\n",
        "High Precision (low false positives): Users get annoyed if legitimate emails go to spam.\n",
        "\n",
        "High Recall (low false negatives): Users don't want to see spam in their inbox.\n",
        "\n",
        "The optimal balance depends on business priorities. Often, a slightly lower precision might be acceptable if it means catching significantly more spam (higher recall).\n",
        "\n",
        "5. Business Impact of the Solution\n",
        "Implementing an effective email spam classification solution has significant positive business impacts:\n",
        "\n",
        "Enhanced User Experience and Productivity:\n",
        "\n",
        "Reduced Inbox Clutter: Users receive fewer unwanted emails, making their inboxes cleaner and easier to manage.\n",
        "\n",
        "Time Savings: Employees spend less time sifting through spam, allowing them to focus on legitimate and productive tasks.\n",
        "\n",
        "Improved Trust: Users trust the email service more when it effectively filters out malicious or irrelevant content.\n",
        "\n",
        "Security and Risk Mitigation:\n",
        "\n",
        "Protection Against Phishing and Malware: Spam often contains phishing attempts, malware, or ransomware. An effective filter significantly reduces the risk of employees falling victim to these attacks, protecting company data and systems.\n",
        "\n",
        "Compliance: Helps in adhering to data privacy and security regulations by minimizing exposure to harmful content.\n",
        "\n",
        "Resource Optimization:\n",
        "\n",
        "Reduced Bandwidth and Storage: Less spam means less data being transmitted and stored on company servers, leading to cost savings on infrastructure.\n",
        "\n",
        "Lower Support Costs: Fewer spam-related issues (e.g., virus infections, phishing incidents) translate to lower IT support demands.\n",
        "\n",
        "Data Quality and Analytics:\n",
        "\n",
        "By having cleaner email data, any downstream analytics or processes (e.g., customer service analysis, internal communication trends) will be more accurate and reliable.\n",
        "\n",
        "In conclusion, a well-designed email spam classification system, leveraging appropriate preprocessing, a robust model like Multinomial Naïve Bayes, techniques to handle class imbalance, and evaluated with suitable metrics like ROC-AUC, not only improves the daily experience of users but also delivers substantial benefits in terms of security, productivity, and operational efficiency for the company."
      ],
      "metadata": {
        "id": "Yla-gAQ4FtGA"
      }
    }
  ]
}